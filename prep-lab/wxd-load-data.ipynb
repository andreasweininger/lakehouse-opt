{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf8dec3",
   "metadata": {},
   "source": [
    "## Load Lab Data\n",
    "\n",
    "This notebook walks through the process of loading wikipedia articles used in the lab into a watsonx.data relational database table. We use the [wikipedia python library](https://pypi.org/project/wikipedia/) to retrieve the wikipedia articles. We then create a table in the database to store the article. Finally, we load the article into the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080c362",
   "metadata": {},
   "source": [
    "#### Fetch wikipedia article\n",
    "\n",
    "Code is provided for searching wikipedia articles as well as fetching a specific article by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0feaa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "# fetch wikipedia articles\n",
    "articles = {\n",
    "    'Nobel price in literature': None, \n",
    "    '2023 Nobel price in literature': 72508137,\n",
    "    '2024 Nobel price in literature': 75098159\n",
    "}\n",
    "for k,v in articles.items():\n",
    "    if v:\n",
    "        article = wikipedia.page(pageid=v)\n",
    "    else:\n",
    "        article = wikipedia.page(k)\n",
    "    articles[k] = article.content\n",
    "    print(f\"Successfully fetched {k}\")\n",
    "\n",
    "print(f\"Successfully fetched {len(articles)} articles \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96792953",
   "metadata": {},
   "source": [
    "## Load wikipedia article into watsonx.data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b70849-6f29-41dd-ad9d-2ea7acffd9bd",
   "metadata": {},
   "source": [
    "#### Connect to watsonx.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4513c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import wxd_utils\n",
    "\n",
    "conf=wxd_utils.load_conf()\n",
    "print(conf)\n",
    "\n",
    "wxd_engine = wxd_utils.connect_wxd(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723dc88-a7a3-4931-aa16-eb33e679a1be",
   "metadata": {},
   "source": [
    "### Create Schema in watsonx.data Hive Bucket to store wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f066915",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "  create_schema_result = pd.read_sql(\"\"\"\n",
    "\n",
    "    CREATE SCHEMA hive_data.watsonxai WITH ( location = 's3a://hive-bucket/watsonx_ai')\n",
    "\n",
    "    \"\"\", wxd_engine)\n",
    "  \n",
    "except sqlalchemy.exc.SQLAlchemyError as e:\n",
    "  print(\"Error creating schema:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90139858",
   "metadata": {},
   "source": [
    "### Create table to hold wikipedia data in schema from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    create_table_result = pd.read_sql(\"\"\"\n",
    "\n",
    "        CREATE TABLE hive_data.watsonxai.wikipedia\n",
    "        (\n",
    "            \"id\" varchar,\n",
    "            \"text\" varchar, \n",
    "            \"title\" varchar  )\n",
    "        WITH (\n",
    "            format = 'PARQUET'\n",
    "        )\n",
    "     \n",
    "    \"\"\", wxd_engine)\n",
    "  \n",
    "except sqlalchemy.exc.SQLAlchemyError as e:\n",
    "  print(\"Error creating table:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c578d6",
   "metadata": {},
   "source": [
    "### Chunk and insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk data\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "split_articles = {}\n",
    "for k,v in articles.items():\n",
    "    split_articles[k] = split_into_chunks(v, 225)\n",
    "\n",
    "# Insert data\n",
    "for article_title, article_chunks in split_articles.items():\n",
    "\n",
    "    for i, chunk in enumerate(article_chunks):\n",
    "            \n",
    "            escaped_chunk = chunk.replace(\"'\", \"''\").replace(\"%\", \"%%\")\n",
    "            insert_stmt = f\"insert into hive_data.watsonxai.wikipedia values ('{i+1}', '{escaped_chunk}', '{article_title}')\"\n",
    "            \n",
    "            with wxd_engine.connect() as connection:\n",
    "                connection.execute(insert_stmt)\n",
    "            print(f\"{article_title} {i+1}/{len(article_chunks)} INSERTED\")\n",
    "            \n",
    "    print(f\"{article_title} DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1cd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm data inserted\n",
    "\n",
    "wiki_articles = pd.read_sql(\"select * from hive_data.watsonxai.wikipedia\", wxd_engine)\n",
    "wiki_articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
